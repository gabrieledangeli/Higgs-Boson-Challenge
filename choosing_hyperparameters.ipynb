{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION FOR DIFFERENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "from implementations import *\n",
    "from crossvalidation import *\n",
    "from preprocessing import *\n",
    "from dataset_splitting import *\n",
    "from feature_engineering import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading train data\n",
    "\n",
    "filename = 'train.csv'\n",
    "data_folder = './data/'\n",
    "file_path = data_folder + filename\n",
    "y,tx,ids,features = load_train_data(file_path)\n",
    "\n",
    "# Computing preprocessing routine\n",
    "\n",
    "list_subsets, list_features, y_0, y_1, y_2_3, columns_to_drop_in_subsets = preprocessing(tx,y,ids,features)\n",
    "\n",
    "for idx in range(3):\n",
    "    list_subsets[idx],mean,std = standardize(list_subsets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to introduce interaction factors between variables during the preprocessing routine. \n",
    "However, there is not statistical significance that multiplying variables with trigonometric functions \n",
    "may improve the model performance. Therefore, since trigonometric values are the last columns of each \n",
    "dataset, we save in a list how many columns are related to trigonometric values in each subset in order \n",
    "not to multiply columns with them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_trig_features=[2,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-6,-3,5)\n",
    "degrees = [3,5,7]\n",
    "k_fold = 4\n",
    "gamma = 0.1\n",
    "max_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining lists to save optimal degrees and lambdas for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lambdas = [0]*3\n",
    "optimal_degrees = [1]*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00018 with a test rmse of 0.335. The best degree is 7.0\n"
     ]
    }
   ],
   "source": [
    "optimal_degrees[0], optimal_lambdas[0], best_rmse = cross_validation_demo_ridge(y_0, list_subsets[0], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_1 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00018 with a test rmse of 0.371. The best degree is 7.0\n"
     ]
    }
   ],
   "source": [
    "optimal_degrees[1], optimal_lambdas[1], best_rmse = cross_validation_demo_ridge(y_1, list_subsets[1], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_2_3 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00003 with a test rmse of 0.347. The best degree is 7.0\n"
     ]
    }
   ],
   "source": [
    "optimal_degrees[2], optimal_lambdas[2], best_rmse = cross_validation_demo_ridge(y_2_3, list_subsets[2], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.360. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_0[:50000], list_subsets[0][:50000], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_1 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.419. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_1, list_subsets[1], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_2_3 for regularized logistc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.377. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_2_3, list_subsets[2], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### COMPUTING ACCURACY FOR RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Computing accuracy for ridge regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_outputs = [y_0,y_1,y_2_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8356289020064241\n",
      "std train accuracy: 0.00036995549985801624\n",
      "Average test accuracy: 0.8351266427558743\n",
      "std train accuracy: 0.0008986234722572053\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.00018,0.00018,0.00003],[7,7,7],how_many_trig_features,pred_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING ACCURACY FOR REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing accuracy for regularized logistic regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8320193575709321\n",
      "std train accuracy: 0.00046693834276353334\n",
      "Average test accuracy: 0.8319370556540728\n",
      "std train accuracy: 0.0010278248657757754\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.0000, 0.0000, 0.0000],[3,3,3],how_many_trig_features, pred_threshold=0.55,method = 'logistic',gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING ACCURACY FOR LEAST SQUARES USING NORMAL EQUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since degree = 7 proved to be a good choice for ridge regression, we decide to test the accuracy for least squares after \n",
    "computing a polynomial expansion up to such degree.\n",
    "<br/>\n",
    "Notice that values of lambdas are useless in the following case. Value of gamma is chosen in order not to move too much along the gradient direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8337482563392427\n",
      "std train accuracy: 0.0004043635190978398\n",
      "Average test accuracy: 0.8333355555259263\n",
      "std train accuracy: 0.0009857922860326252\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.0000, 0.0000, 0.0000],[7,7,7],how_many_trig_features,\n",
    "                 pred_threshold=0.55,method = 'ls_normal_equations',gamma = 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e9c25df0253b19710fd2eabc0119b804c820fc74e8ba938ebab686d76fc4dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
